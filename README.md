## I. Introduction
Scientists at Los Alamos Laboratory have recently found a use for massive amounts of data generated by a “constant tremor” of fault lines where earthquakes are most common **[1-3]**. This data has previously been disregarded as noise. However, now, it has been proven useful through the lens of Machine Learning (ML) **[1-2]**. Following their recent publications, our goal is to build _Machine Learning regression models for the Laboratory Earthquake problem_ that if applied to real data, might help predict earthquakes before they happen

#### Environment Setup
<details><summary>CLICK TO EXPAND</summary>
<p>
  
```markdown
import os
from scipy import ndimage, misc
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston, load_diabetes, load_digits, load_breast_cancer
from keras.datasets import mnist
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
import statistics 
```

</p>
</details>

## II. Feature Extraction
%describe the process%

```python
your code here
```
To access original data set, see [LANL Earthquake Prediction Data Set](https://www.kaggle.com/c/LANL-Earthquake-Prediction/data)

To access processed data set, see [Data Extracted](extract_train_full.csv)

## IIIa. Linear Regression
#### Transform data
<details><summary>CLICK TO EXPAND</summary>
<p>
  
```python
target = pd.read_csv("extract_label_Jul08.csv", delimiter = ',')
target = target.as_matrix()
target = target[:,1]

features = pd.read_csv("extract_train_Jul08.csv", delimiter = ',')
features = features.as_matrix()
features = features[:, 1:17]
```

</p>
</details>

#### Plot Features Methods
<details><summary>CLICK TO EXPAND</summary>
<p>
  
```python
def plot_features(features, reg, features_poly, number):
    plt.ylabel("Target Values")
    plt.xlabel("Feature #" + str(number))
    plt.scatter(features[:,number-1], target, s=10, c='b', marker="s", label='original')
    plt.scatter(features[:,number-1], reg.predict(features_poly),  s=10, c='r', marker="o", label='predicted')
    plt.legend(loc='upper right')
```

</p>
</details>

#### Kfold Cross Validation for Linear and Polynomial Regression
<details><summary>CLICK TO EXPAND</summary>
<p>
  
```python
def K_Fold(features, target, numfolds, classifier):

    kf = KFold(n_splits=numfolds)
    kf.get_n_splits(features)

    i = 0
    mae = np.zeros(numfolds-1) 
    for train_index, test_index in kf.split(features):
        features_train, features_test = features[train_index], features[test_index]
        target_train, target_test = target[train_index], target[test_index]
    
        poly = PolynomialFeatures(degree=2)
        features_poly_train = features_train 
        features_poly_test = features_test
        if classifier == "polynomial" :
            features_poly_train = poly.fit_transform(features_train)
            features_poly_test = poly.fit_transform(features_test)
        elif classifier == "linear":
            features_poly_train = features_train 
            features_poly_test = features_test
        
        reg = LinearRegression().fit(features_poly_train, target_train)
    
        i = i+1
        if (i < numfolds):
            mae[i-1] = mean_absolute_error(target_test, reg.predict(features_poly_test))
            
    print("The average values of mean absolute error is:", (sum(mae)/(numfolds-1)))
    print("Variance of mean absolute error is % s" %(statistics.variance(mae))) 
```

</p>
</details>

#### Perform linear regression on the data

```python
reg = LinearRegression().fit(features, target)

print("The loss values is: ", mean_absolute_error(target, reg.predict(features)))
```
The loss values is:  2.110853811043013

#### Perfom Kfold cross validation on the data

```python
K_Fold(features,target, 5, "linear")
K_Fold(features,target, 10, "linear")
K_Fold(features,target, 50, "linear")
K_Fold(features,target, 100, "linear")
```
The average values of mean absolute error is: 2.11648228726888  
Variance of mean absolute error is 0.3037948123611948  
The average values of mean absolute error is: 2.32764647869577  
Variance of mean absolute error is 0.6064364572425152  
The average values of mean absolute error is: 2.215050705247842  
Variance of mean absolute error is 1.632732053904914  
The average values of mean absolute error is: 2.1840056400348913  
Variance of mean absolute error is 1.7970172144011285  

#### Graph the results

<details><summary>CLICK TO EXPAND</summary>
<p>
 
```python
fig, axes = plt.subplots(nrows=3, ncols=2)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)

for i in range(6):
    plt.subplot(32*10 +(i+1))
    plot_features(features, reg, features, i+1)

fig, axes = plt.subplots(nrows=3, ncols=2)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)

for i in range(6):
    plt.subplot(32*10 +(i+1))
    plot_features(features, reg, features, i+7)

fig, axes = plt.subplots(nrows=3, ncols=2)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)

for i in range(6):
    plt.subplot(32*10 +(i+1))
    plot_features(features, reg, features, i+13)
```

</p>
</details>

![Linear Regression](https://github.com/hoangtung167/cx4240/blob/master/Linear%20Regression.png)

#### Analysis

## IIIb. Polynomial Regression
#### Perform polynomial regression on the data
 
```python
poly = PolynomialFeatures(degree=2)
features_poly = poly.fit_transform(features)
reg = LinearRegression().fit(features_poly, target)

print("The loss values is: ", mean_absolute_error(target, reg.predict(features_poly)))
```
The loss values is:  1.985654086901071

#### Perfom Kfold cross validation on the data

```python
K_Fold(features,target, 5, "polynomial")
K_Fold(features,target, 10, "polynomial")
K_Fold(features,target, 50, "polynomial")
K_Fold(features,target, 100, "polynomial")
```
The average values of mean absolute error is: 2.2161712125633586  
Variance of mean absolute error is 0.36444471639163495  
The average values of mean absolute error is: 2.4379615680536126  
Variance of mean absolute error is 0.7217988794498568  
The average values of mean absolute error is: 2.311128151349101  
Variance of mean absolute error is 1.7229855562955887  
The average values of mean absolute error is: 2.2432512038106136  
Variance of mean absolute error is 1.9431429755822602  

#### Graph the results

<details><summary>CLICK TO EXPAND</summary>
<p>
  
```python
fig, axes = plt.subplots(nrows=3, ncols=2)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)

for i in range(6):
    plt.subplot(32*10 +(i+1))
    plot_features(features, reg, features_poly, i+1)

fig, axes = plt.subplots(nrows=3, ncols=2)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)

for i in range(6):
    plt.subplot(32*10 +(i+1))
    plot_features(features, reg, features_poly, i+7)

fig, axes = plt.subplots(nrows=3, ncols=2)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)

for i in range(4):
    plt.subplot(32*10 +(i+1))
    plot_features(features, reg, features_poly, i+13)
```

</p>
</details>

![Polynomial Regression](https://github.com/hoangtung167/cx4240/blob/master/Polynomial%20Regression.png)


#### Analysis

## IV. Decision Tree/ Random Forest

## V. Deep Learning with Fully Connected, LSTM or 1D-CNN layer

## I. Introduction
Scientists at Los Alamos Laboratory have recently found a use for massive amounts of data generated by a “constant tremor” of fault lines where earthquakes are most common **[1-3]**. This data has previously been disregarded as noise. However, now, it has been proven useful through the lens of Machine Learning (ML) **[1-2]**. Following their recent publications, our goal is to build _Machine Learning regression models for the Laboratory Earthquake problem_ that if applied to real data, might help predict earthquakes before they happen

#### Environment Setup
<details><summary>CLICK TO EXPAND</summary>
<p>
  
```markdown
import os
from scipy import ndimage, misc
from matplotlib import pyplot as plt
import numpy as np
from sklearn.datasets import load_boston, load_diabetes, load_digits, load_breast_cancer
from keras.datasets import mnist
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
```

</p>
</details>

## II. Feature Extraction
%describe the process%

```python
your code here
```
To access original data set, see [LANL Earthquake Prediction Data Set](https://www.kaggle.com/c/LANL-Earthquake-Prediction/data)

To access processed data set, see [Data Extracted](extract_train_full.csv)

## IIIa. Linear Regression
#### Transform data
<details><summary>CLICK TO EXPAND</summary>
<p>
  
```python
train = pd.read_csv("extract_train_full.csv", delimiter = ',')
dataset = train.as_matrix()
features = dataset[0:dataset.shape[0], 1:6]
target = dataset[0:dataset.shape[0], 6]
```

</p>
</details>

#### Perform linear regression on the data

```python
reg = LinearRegression().fit(features, target)
print(reg.score(features, target))
reg.coef_

fig, axes = plt.subplots(nrows=2, ncols=3)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)
```

### Graph the results

<details><summary>CLICK TO EXPAND</summary>
<p>
 
```python
plt.subplot(321)
plt.ylabel("Target Values")
plt.xlabel("Feature #1")
plt.scatter(features[:,0], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,0], reg.predict(features),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(322)
plt.ylabel("Target Values")
plt.xlabel("Feature #2")
plt.scatter(features[:,1], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,1], reg.predict(features),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(323)
plt.ylabel("Target Values")
plt.xlabel("Feature #3")
plt.scatter(features[:,2], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,2], reg.predict(features),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(324)
plt.ylabel("Target Values")
plt.xlabel("Feature #4")
plt.scatter(features[:,3], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,3], reg.predict(features),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(325)
plt.ylabel("Target Values")
plt.xlabel("Feature #5")
plt.scatter(features[:,4], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,4], reg.predict(features),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');
```

</p>
</details>

![Linear Regression](https://github.com/hoangtung167/cx4240/blob/master/Linear%20Regression.png)

#### Regression Score
The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.

Regression Score of Linear Regression is 0.4754292221704254.

#### Analysis

## IIIb. Polynomial Regression
#### Perform polynomial regression on the data


 
```python
poly = PolynomialFeatures(degree=2)
features_poly = poly.fit_transform(features)
reg = LinearRegression().fit(features_poly, target)
print(reg.score(features_poly, target))
```

<details><summary>CLICK TO EXPAND</summary>
<p>
  
```python
fig, axes = plt.subplots(nrows=2, ncols=3)
fig.tight_layout()
fig.subplots_adjust(left=0.1, bottom=-1.2, right=0.9, top=0.9, wspace=0.4, hspace=0.2)

plt.subplot(321)
plt.ylabel("Target Values")
plt.xlabel("Feature #1")
plt.scatter(features[:,0], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,0], reg.predict(features_poly),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(322)
plt.ylabel("Target Values")
plt.xlabel("Feature #2")
plt.scatter(features[:,1], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,1], reg.predict(features_poly),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(323)
plt.ylabel("Target Values")
plt.xlabel("Feature #3")
plt.scatter(features[:,2], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,2], reg.predict(features_poly),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(324)
plt.ylabel("Target Values")
plt.xlabel("Feature #4")
plt.scatter(features[:,3], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,3], reg.predict(features_poly),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');

plt.subplot(325)
plt.ylabel("Target Values")
plt.xlabel("Feature #5")
plt.scatter(features[:,4], target, s=10, c='b', marker="s", label='original')
plt.scatter(features[:,4], reg.predict(features_poly),  s=10, c='r', marker="o", label='predicted')
plt.legend(loc='upper right');
```

</p>
</details>

![Polynomial Regression](https://github.com/hoangtung167/cx4240/blob/master/Polynomial%20Regression.png)

#### Regression Score
The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.

Regression Score of Linear Regression is 0.4909749205128894.

#### Analysis

## IV. Decision Tree/ Random Forest

## V. Deep Learning with Fully Connected, LSTM or 1D-CNN layer
